{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Resume Screening with NLP and Machine Learning\n",
    "\n",
    "This Jupyter Notebook extracts text from resumes (PDFs), preprocesses the text, and ranks resumes based on how well they match a given job description using **TF-IDF (Term Frequency - Inverse Document Frequency)** and **cosine similarity**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.25.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aksha\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aksha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aksha\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "%pip install pandas numpy PyMuPDF nltk scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download stopwords dataset\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akshay Bhujbal\n",
      "Pune | akshay.bhujbal16@gmail.com | 07499902809 | github.com/AkshayBhujbal1995\n",
      "linkedin.com/in/akshay-1995-bhujbal\n",
      "Professional Summary\n",
      "• Data Analyst with expertise in data visualization, predictive analytics, and business intelligence.\n",
      "• Proficient in Python, SQL, and Machine Learning, with a strong ability to extract meaningful insights from\n",
      "complex datasets.\n",
      "• Adept at designing automated dashboards, optimizing business processes, and leveraging AI techniques to\n",
      "drive decision-making.\n",
      "• Passionate about utilizing data-driven solutions to enhance business performance and efficiency.\n",
      "Technologies\n",
      "Programming Languages: Python, SQL, R\n",
      "Data Analysis & Visualization: Pandas, NumPy, Matplotlib, Seaborn, Power BI, Tableau\n",
      "Machine Learning & AI: Supervised and Unsupervised Learning, Neural Networks, TensorFlow, Scikit-learn\n",
      "Statistical & Business Analytics: SAS (Base) Hypothesis Testing, Predictive Modeling, Anomaly Detection\n",
      "Databases & ETL: SQL, MySQL, Data Warehousing, ET\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Extract Text from PDFs\n",
    "\n",
    "# We define a function `extract_text_from_pdf` to extract text from resumes stored as PDFs.\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file using PyMuPDF (fitz).\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Define path to your CV\n",
    "# Use your own path \n",
    "cv_path = r\"C:\\\\Users\\\\aksha\\\\Downloads\\\\Data_Analyst_Cv\\\\Akshay-Bhujbal-Resume.pdf\"  \n",
    "\n",
    "# Extract text from your resume\n",
    "resume_text = extract_text_from_pdf(cv_path)\n",
    "\n",
    "# Print the first 1000 characters of extracted text\n",
    "print(resume_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akshay bhujbal pune akshaybhujbal16gmailcom 07499902809 githubcomakshaybhujbal1995 linkedincominakshay1995bhujbal professional summary data analyst expertise data visualization predictive analytics business intelligence proficient python sql machine learning strong ability extract meaningful insights complex datasets adept designing automated dashboards optimizing business processes leveraging ai techniques drive decisionmaking passionate utilizing datadriven solutions enhance business performan\n",
      "looking data scientist experience python machine learning nlp sql\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Preprocess Text\n",
    "\"\"\"\n",
    "We define a function `preprocess_text` that:\n",
    "- Converts text to lowercase\n",
    "- Removes punctuation\n",
    "- Removes stopwords (common words like \"the\", \"is\", \"and\" that don’t add much meaning)\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and preprocesses text.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Example job description\n",
    "job_description = \"\"\"\n",
    "We are looking for a Data Scientist with experience in Python, Machine Learning, NLP, and SQL.\n",
    "\"\"\"\n",
    "\n",
    "# Clean the texts\n",
    "clean_resume_text = preprocess_text(resume_text)\n",
    "clean_job_description = preprocess_text(job_description)\n",
    "\n",
    "# Print cleaned text samples\n",
    "print(clean_resume_text[:500])\n",
    "print(clean_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume Match Score: 0.31\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Calculate Resume Similarity Score\n",
    "\n",
    "# We use **TF-IDF** to convert text into numerical vectors and \n",
    "# then measure similarity using cosine similarity.\n",
    "\n",
    "\n",
    "# Convert job description and resumes to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([clean_job_description, clean_resume_text])\n",
    "\n",
    "# Compute similarity (closer to 1 = better match)\n",
    "similarity_score = cosine_similarity(vectors)[0][1]\n",
    "print(f\"Resume Match Score: {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akshay-Bhujbal_Data-Analyst_CV..pdf: 0.27\n",
      "Akshay_Bhujbal_DA_CV.pdf: 0.26\n",
      "AkshayBhujbalResume.pdf: 0.25\n",
      "Akshay_Bhujbal_Data_Analyst_Resume.pdf: 0.23\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Process Multiple Resumes in a Folder\n",
    "\n",
    "# This section processes multiple resumes from a folder and ranks them based on job match score.\n",
    "\n",
    "# Folder containing resumes\n",
    "resume_folder = r\"C:\\\\Users\\\\aksha\\\\Downloads\\\\Data_Analyst_Cv\\\\resumes\"\n",
    "\n",
    "# Process multiple resumes\n",
    "resume_scores = {}\n",
    "for resume_file in os.listdir(resume_folder):\n",
    "    resume_text = extract_text_from_pdf(os.path.join(resume_folder, resume_file))\n",
    "    clean_resume_text = preprocess_text(resume_text)\n",
    "    \n",
    "    # Convert to TF-IDF vectors\n",
    "    vectors = vectorizer.transform([clean_job_description, clean_resume_text])\n",
    "    similarity_score = cosine_similarity(vectors)[0][1]\n",
    "    \n",
    "    resume_scores[resume_file] = similarity_score\n",
    "\n",
    "# Sort resumes by highest match score\n",
    "sorted_resumes = sorted(resume_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked resumes\n",
    "for resume, score in sorted_resumes:\n",
    "    print(f\"{resume}: {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
